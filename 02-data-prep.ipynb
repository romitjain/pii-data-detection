{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e95de7-7801-401d-b098-fee169c373a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils import label2id, chunk_examples, data_augmenter, tokenizer_and_align, filter_data\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets, DatasetDict\n",
    "from model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5cc1d-0d77-4ac7-9937-89479c9a8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = './data/train.json'\n",
    "fn = './data/augmented_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350fb50-05ad-434a-ad43-f6a02fe2e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4343ed-e8d8-446f-8ae3-0461d3386a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model('sileod/deberta-v3-large-tasksource-nli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c98e32-e662-42cb-b2f9-1d726afed775",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127cc95-78ea-4eaf-8840-878b0cddf601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f459d6-db49-4af3-a656-505fd39cb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = x.map(tokenizer_and_align, num_proc=16, fn_kwargs={'tokenizer': tokenizer})\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f074f46-acea-4820-9c73-32799a7d5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.filter(filter_data, num_proc=16, fn_kwargs={'p': 0.95})\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2754da-fe6b-4a09-b606-34423deaf727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = x.map(\n",
    "    chunk_examples,\n",
    "    num_proc=1,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=x.column_names,\n",
    "    fn_kwargs={'max_len': 2048, 'buffer': 0}\n",
    ")\n",
    "\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f803cca-6d33-4433-81e8-9f05a8838918",
   "metadata": {},
   "source": [
    "### Dataset saving and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d01dda-7e60-4e34-9b1d-7fd0b5095837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.save_to_disk('./data/processed/data/')\n",
    "ds.save_to_disk('./data/processed/augmented_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417cd4c-512c-41fa-93c3-09250f5ebed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0 = load_from_disk('./data/processed/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb4a0d-fb0a-43ca-aba6-cfe80c265bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final = concatenate_datasets([ds0, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6f997-55c4-4eb4-b9fb-879a5e0f01c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_idx = np.unique(ds['document_id'] + ds0['document_id'])\n",
    "val_idx = np.random.choice(\n",
    "    unique_idx,\n",
    "    size=int(0.2 * unique_idx.shape[0]),\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "train_idx = np.setdiff1d(unique_idx, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef99f86-2077-45d1-a996-2bf00eaad95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_sample(example): return True if example['document_id'] in train_idx else False\n",
    "def get_val_sample(example): return True if example['document_id'] in val_idx else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39aeef-3dbc-42ce-9a70-c18ccf00a515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = ds_final.filter(get_train_sample, num_proc=16)\n",
    "val_ds = ds_final.filter(get_val_sample, num_proc=16)\n",
    "\n",
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba643df-da9b-43e7-b7f7-0534fe6e9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save = DatasetDict({'train': train_ds, 'val': val_ds})\n",
    "ds_to_save.save_to_disk('./data/processed/dataset_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d453dc-046e-4127-a928-3bc9f407993c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a61da-83b3-40b3-9d5d-cdb24c9b20a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(data))\n",
    "start= np.random.randint(0, len(data[idx]['tokens']))\n",
    "buffer = 20\n",
    "# idx, start = 0, 0\n",
    "\n",
    "temp = data[idx]\n",
    "\n",
    "for tokens, labels, ws in zip(\n",
    "    temp['tokens'][start: start+buffer],\n",
    "    temp['labels'][start: start+buffer],\n",
    "    temp['trailing_whitespace'][start: start+buffer]\n",
    "):\n",
    "    if labels == 'O':\n",
    "        continue\n",
    "    \n",
    "    local = {'tokens': [tokens], 'labels': [labels], 'trailing_whitespace': [ws]}\n",
    "    ans = tokenizer_and_align(local, tokenizer)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Original: {tokens} {labels}\n",
    "    Transformed: {ans['tokens']} {ans['aligned_tokens']['input_ids']} {ans['aligned_labels']}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5232ad-3f90-47b1-ac0a-4117c71276f4",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fea12-fa70-4eb1-89df-97c5443dc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from copy import deepcopy\n",
    "from data_utils import random_augmentation\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8ff60-83f2-46a0-80c0-29165abbb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bf288-fbe9-4282-b830-ed316b745a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For every document, whenever a !'O' type token comes\n",
    "# wait for 'O' token to come again, until then collect the tokens\n",
    "# Concatenate the tokens, use the label of the last token\n",
    "# and send for data augmentation.\n",
    "# Replace the elements by the new augmented elements and add in the augmented docs\n",
    "# and move on to the next document\n",
    "\n",
    "augmented_docs = []\n",
    "\n",
    "for idx, _ in enumerate(data):\n",
    "    tokens_to_augment = []\n",
    "    ids_to_replace = []\n",
    "    prev_label = None\n",
    "    flag=0\n",
    "    \n",
    "    for pos, it in enumerate(zip(\n",
    "        data[idx]['tokens'],\n",
    "        data[idx]['trailing_whitespace'],\n",
    "        data[idx]['labels']\n",
    "    )):\n",
    "        token, ws, label = it\n",
    "        \n",
    "        if label != 'O':\n",
    "            tokens_to_augment.append(f' {token}' if ws else token)\n",
    "            ids_to_replace.append(pos)\n",
    "            prev_label = label\n",
    "\n",
    "        if label == 'O' and tokens_to_augment:\n",
    "            result = random_augmentation(\n",
    "                ' '.join(tokens_to_augment).strip(),\n",
    "                prev_label\n",
    "            )\n",
    "            result = spacy_tokenize(result)\n",
    "\n",
    "            temp_doc = deepcopy(data[idx])\n",
    "            for id, replace_token in zip(ids_to_replace, result):\n",
    "                temp_doc['tokens'][id] = replace_token\n",
    "\n",
    "            augmented_docs.append(temp_doc)\n",
    "\n",
    "            tokens_to_augment = []\n",
    "            ids_to_replace = []\n",
    "            prev_label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f52ae-9182-42ad-9b1e-fff23fa2adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/augmented_data.json', 'w') as fp:\n",
    "    json.dump(augmented_docs, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd905eb-1d52-4030-9794-d4f01e621f9e",
   "metadata": {},
   "source": [
    "### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924652f-e3e9-4a3a-baee-0778e085d938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[0]['aligned_tokens']['attention_mask'][710:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38a8c9-98da-4b75-92c0-181a176ab1d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[0]['aligned_tokens']['input_ids'][710:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784d80a-9ec0-4d85-9d3b-a9754cf6178d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[0]['aligned_labels'][710:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83008dca-bc72-4831-a0dc-b2ee53613f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_row(example):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    labels = []\n",
    "    targets = []\n",
    "    idx = 0\n",
    "    for t, l, ws in zip(example[\"tokens\"], example[\"labels\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        labels.extend([l]*len(t))\n",
    "        token_map.extend([idx]*len(t))\n",
    "\n",
    "        if l in config['target_cols']:  \n",
    "            targets.append(1)\n",
    "        else:\n",
    "            targets.append(0)\n",
    "        \n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "            token_map.append(-1)\n",
    "        idx += 1\n",
    "\n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=2048)  # Adjust max_length if needed\n",
    "     \n",
    "    target_num = sum(targets)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "\n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        if start_idx == 0 and end_idx == 0: \n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "        \n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "        try:\n",
    "            token_labels.append(label2id[labels[start_idx]])\n",
    "        except:\n",
    "            continue\n",
    "    length = len(tokenized.input_ids)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized.input_ids,\n",
    "        \"attention_mask\": tokenized.attention_mask,\n",
    "        \"offset_mapping\": tokenized.offset_mapping,\n",
    "        \"labels\": token_labels,\n",
    "        \"length\": length,\n",
    "        \"target_num\": target_num,\n",
    "        \"group\": 1 if target_num > 0 else 0,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb37b21b-097d-4866-8d5e-baf6ec8e8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'target_cols': ['TARGET'],\n",
    "    'valid_stride': False,\n",
    "    'max_length': 512\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9fd65-9e3d-489f-8265-8372f0844839",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = x[9]\n",
    "result = tokenize_row(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e6c56-7bb9-475c-8519-e43c5893adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(result['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f774d8fd-3b0a-4616-be57-15b5dd83c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in zip(result['input_ids'], result['labels']):\n",
    "    if l != 0:\n",
    "        print(f'{tokenizer.decode(i)} => {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f75d811-5741-44ee-aaab-de2753ae2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in zip(ds[9]['input_ids'], ds[9]['labels']):\n",
    "    if l != 0 and l != -100:\n",
    "        print(f'{tokenizer.decode(i)} => {l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24cf389-34ed-414c-826c-a7948fce7cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer('Nathalie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2187644-fbb9-4b98-a75a-da1e04e24b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(x[0]['aligned_tokens']['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e797c7f-b496-470e-9dfb-b21c6aa83d11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcecf188-deba-423d-8bb5-81dd126c14d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c94fd-4c7b-44ae-8adc-0d15a6ac02d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
