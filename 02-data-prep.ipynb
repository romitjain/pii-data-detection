{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e95de7-7801-401d-b098-fee169c373a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils import label2id, chunk_examples, data_augmenter, tokenizer_and_align, filter_data\n",
    "from datasets import Dataset\n",
    "from model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5cc1d-0d77-4ac7-9937-89479c9a8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = './data/train.json'\n",
    "fn = './data/augmented_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350fb50-05ad-434a-ad43-f6a02fe2e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4343ed-e8d8-446f-8ae3-0461d3386a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c98e32-e662-42cb-b2f9-1d726afed775",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127cc95-78ea-4eaf-8840-878b0cddf601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f459d6-db49-4af3-a656-505fd39cb342",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.map(tokenizer_and_align, num_proc=16, fn_kwargs={'tokenizer': tokenizer})\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f074f46-acea-4820-9c73-32799a7d5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.filter(filter_data, num_proc=16, fn_kwargs={'p': 0.95})\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832eec2-56b6-40a9-a542-e62895ec2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\n",
    "    'This is my name: Romit Jain',\n",
    "    max_length=2048,\n",
    "    return_overflowing_tokens=True,\n",
    "    add_special_tokens=False,\n",
    "    is_split_into_words=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e331f-c736-41b6-a5af-47e8f3d6b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer(\n",
    "    'This is my name: Romit Jain'.split(' '),\n",
    "    # max_length=5,\n",
    "    return_overflowing_tokens=True,\n",
    "    add_special_tokens=False,\n",
    "    is_split_into_words=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41e36d-9dc8-4285-8a1c-1741cb90abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.tokens(), t.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2754da-fe6b-4a09-b606-34423deaf727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = x.map(\n",
    "    chunk_examples,\n",
    "    num_proc=1,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=x.column_names,\n",
    "    fn_kwargs={'max_len': 2048}\n",
    ")\n",
    "\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d01dda-7e60-4e34-9b1d-7fd0b5095837",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk('./data/processed/augmented_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d453dc-046e-4127-a928-3bc9f407993c",
   "metadata": {},
   "source": [
    "## Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a61da-83b3-40b3-9d5d-cdb24c9b20a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(data))\n",
    "start= np.random.randint(0, len(data[idx]['tokens']))\n",
    "buffer = 2000\n",
    "# idx, start = 0, 0\n",
    "\n",
    "temp = data[idx]\n",
    "\n",
    "for tokens, labels, ws in zip(\n",
    "    temp['tokens'][start: start+buffer],\n",
    "    temp['labels'][start: start+buffer],\n",
    "    temp['trailing_whitespace'][start: start+buffer]\n",
    "):\n",
    "    if labels == 'O':\n",
    "        continue\n",
    "    \n",
    "    local = {'tokens': [tokens], 'labels': [labels], 'trailing_whitespace': [ws]}\n",
    "    ans = tokenizer_and_align(local, tokenizer)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Original: {tokens} {labels}\n",
    "    Transformed: {ans['tokens']} {ans['aligned_tokens']['input_ids']} {ans['aligned_labels']}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5232ad-3f90-47b1-ac0a-4117c71276f4",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fea12-fa70-4eb1-89df-97c5443dc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from copy import deepcopy\n",
    "from data_utils import random_augmentation\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8ff60-83f2-46a0-80c0-29165abbb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bf288-fbe9-4282-b830-ed316b745a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For every document, whenever a !'O' type token comes\n",
    "# wait for 'O' token to come again, until then collect the tokens\n",
    "# Concatenate the tokens, use the label of the last token\n",
    "# and send for data augmentation.\n",
    "# Replace the elements by the new augmented elements and add in the augmented docs\n",
    "# and move on to the next document\n",
    "\n",
    "augmented_docs = []\n",
    "\n",
    "for idx, _ in enumerate(data):\n",
    "    tokens_to_augment = []\n",
    "    ids_to_replace = []\n",
    "    prev_label = None\n",
    "    flag=0\n",
    "    \n",
    "    for pos, it in enumerate(zip(\n",
    "        data[idx]['tokens'],\n",
    "        data[idx]['trailing_whitespace'],\n",
    "        data[idx]['labels']\n",
    "    )):\n",
    "        token, ws, label = it\n",
    "        \n",
    "        if label != 'O':\n",
    "            tokens_to_augment.append(f' {token}' if ws else token)\n",
    "            ids_to_replace.append(pos)\n",
    "            prev_label = label\n",
    "\n",
    "        if label == 'O' and tokens_to_augment:\n",
    "            result = random_augmentation(\n",
    "                ' '.join(tokens_to_augment).strip(),\n",
    "                prev_label\n",
    "            )\n",
    "            result = spacy_tokenize(result)\n",
    "\n",
    "            temp_doc = deepcopy(data[idx])\n",
    "            for id, replace_token in zip(ids_to_replace, result):\n",
    "                temp_doc['tokens'][id] = replace_token\n",
    "\n",
    "            augmented_docs.append(temp_doc)\n",
    "\n",
    "            tokens_to_augment = []\n",
    "            ids_to_replace = []\n",
    "            prev_label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f52ae-9182-42ad-9b1e-fff23fa2adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/augmented_data.json', 'w') as fp:\n",
    "    json.dump(augmented_docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38a8c9-98da-4b75-92c0-181a176ab1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
