{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e95de7-7801-401d-b098-fee169c373a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_utils import label2id, chunk_examples, data_augmenter, tokenizer_and_align, filter_data\n",
    "from datasets import Dataset, load_from_disk, concatenate_datasets, DatasetDict\n",
    "from model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5cc1d-0d77-4ac7-9937-89479c9a8fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = './data/train.json'\n",
    "fn = './data/augmented_data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350fb50-05ad-434a-ad43-f6a02fe2e8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn, 'r') as fp:\n",
    "    data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4343ed-e8d8-446f-8ae3-0461d3386a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model('dslim/bert-large-NER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c98e32-e662-42cb-b2f9-1d726afed775",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127cc95-78ea-4eaf-8840-878b0cddf601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "x = Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f459d6-db49-4af3-a656-505fd39cb342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = x.map(tokenizer_and_align, num_proc=16, fn_kwargs={'tokenizer': tokenizer})\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f074f46-acea-4820-9c73-32799a7d5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.filter(filter_data, num_proc=16, fn_kwargs={'p': 0.95})\n",
    "print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832eec2-56b6-40a9-a542-e62895ec2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\n",
    "    'This is my name: Romit Jain',\n",
    "    max_length=2048,\n",
    "    return_overflowing_tokens=True,\n",
    "    add_special_tokens=False,\n",
    "    is_split_into_words=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e331f-c736-41b6-a5af-47e8f3d6b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer(\n",
    "    'This is my name: Romit Jain'.split(' '),\n",
    "    # max_length=5,\n",
    "    return_overflowing_tokens=True,\n",
    "    add_special_tokens=False,\n",
    "    is_split_into_words=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41e36d-9dc8-4285-8a1c-1741cb90abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.tokens(), t.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2754da-fe6b-4a09-b606-34423deaf727",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds = x.map(\n",
    "    chunk_examples,\n",
    "    num_proc=1,\n",
    "    batched=True,\n",
    "    batch_size=10,\n",
    "    remove_columns=x.column_names,\n",
    "    fn_kwargs={'max_len': 256}\n",
    ")\n",
    "\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f803cca-6d33-4433-81e8-9f05a8838918",
   "metadata": {},
   "source": [
    "### Dataset saving and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d01dda-7e60-4e34-9b1d-7fd0b5095837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.save_to_disk('./data/processed/data/')\n",
    "ds.save_to_disk('./data/processed/augmented_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417cd4c-512c-41fa-93c3-09250f5ebed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0 = load_from_disk('./data/processed/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb4a0d-fb0a-43ca-aba6-cfe80c265bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final = concatenate_datasets([ds0, ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f651e0b-4e0d-4e89-95c6-aa7e83808891",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([1, 2, 3, 4], size=2, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf6f997-55c4-4eb4-b9fb-879a5e0f01c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_idx = np.unique(ds['document_id'] + ds0['document_id'])\n",
    "val_idx = np.random.choice(\n",
    "    unique_idx,\n",
    "    size=int(0.2 * unique_idx.shape[0]),\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "train_idx = np.setdiff1d(unique_idx, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef99f86-2077-45d1-a996-2bf00eaad95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_sample(example): return True if example['document_id'] in train_idx else False\n",
    "def get_val_sample(example): return True if example['document_id'] in val_idx else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39aeef-3dbc-42ce-9a70-c18ccf00a515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds = ds_final.filter(get_train_sample, num_proc=16)\n",
    "val_ds = ds_final.filter(get_val_sample, num_proc=16)\n",
    "\n",
    "print(len(train_ds), len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba643df-da9b-43e7-b7f7-0534fe6e9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_to_save = DatasetDict({'train': train_ds, 'val': val_ds})\n",
    "ds_to_save.save_to_disk('./data/processed/dataset_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d453dc-046e-4127-a928-3bc9f407993c",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a61da-83b3-40b3-9d5d-cdb24c9b20a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(data))\n",
    "start= np.random.randint(0, len(data[idx]['tokens']))\n",
    "buffer = 2000\n",
    "# idx, start = 0, 0\n",
    "\n",
    "temp = data[idx]\n",
    "\n",
    "for tokens, labels, ws in zip(\n",
    "    temp['tokens'][start: start+buffer],\n",
    "    temp['labels'][start: start+buffer],\n",
    "    temp['trailing_whitespace'][start: start+buffer]\n",
    "):\n",
    "    # if labels == 'O':\n",
    "    #     continue\n",
    "    \n",
    "    local = {'tokens': [tokens], 'labels': [labels], 'trailing_whitespace': [ws]}\n",
    "    ans = tokenizer_and_align(local, tokenizer)\n",
    "\n",
    "    print(f\"\"\"\n",
    "    Original: {tokens} {labels}\n",
    "    Transformed: {ans['tokens']} {ans['aligned_tokens']['input_ids']} {ans['aligned_labels']}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5232ad-3f90-47b1-ac0a-4117c71276f4",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fea12-fa70-4eb1-89df-97c5443dc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from copy import deepcopy\n",
    "from data_utils import random_augmentation\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8ff60-83f2-46a0-80c0-29165abbb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098bf288-fbe9-4282-b830-ed316b745a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For every document, whenever a !'O' type token comes\n",
    "# wait for 'O' token to come again, until then collect the tokens\n",
    "# Concatenate the tokens, use the label of the last token\n",
    "# and send for data augmentation.\n",
    "# Replace the elements by the new augmented elements and add in the augmented docs\n",
    "# and move on to the next document\n",
    "\n",
    "augmented_docs = []\n",
    "\n",
    "for idx, _ in enumerate(data):\n",
    "    tokens_to_augment = []\n",
    "    ids_to_replace = []\n",
    "    prev_label = None\n",
    "    flag=0\n",
    "    \n",
    "    for pos, it in enumerate(zip(\n",
    "        data[idx]['tokens'],\n",
    "        data[idx]['trailing_whitespace'],\n",
    "        data[idx]['labels']\n",
    "    )):\n",
    "        token, ws, label = it\n",
    "        \n",
    "        if label != 'O':\n",
    "            tokens_to_augment.append(f' {token}' if ws else token)\n",
    "            ids_to_replace.append(pos)\n",
    "            prev_label = label\n",
    "\n",
    "        if label == 'O' and tokens_to_augment:\n",
    "            result = random_augmentation(\n",
    "                ' '.join(tokens_to_augment).strip(),\n",
    "                prev_label\n",
    "            )\n",
    "            result = spacy_tokenize(result)\n",
    "\n",
    "            temp_doc = deepcopy(data[idx])\n",
    "            for id, replace_token in zip(ids_to_replace, result):\n",
    "                temp_doc['tokens'][id] = replace_token\n",
    "\n",
    "            augmented_docs.append(temp_doc)\n",
    "\n",
    "            tokens_to_augment = []\n",
    "            ids_to_replace = []\n",
    "            prev_label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f52ae-9182-42ad-9b1e-fff23fa2adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/augmented_data.json', 'w') as fp:\n",
    "    json.dump(augmented_docs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38a8c9-98da-4b75-92c0-181a176ab1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
