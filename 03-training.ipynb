{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e95de7-7801-401d-b098-fee169c373a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torch.cuda import empty_cache\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from data_utils import label2id, id2label\n",
    "from datasets import load_from_disk\n",
    "from model import get_model\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7126b-2e19-4dd7-ad2b-11cce959e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(list(label2id.keys()))\n",
    "\n",
    "def stack(x, p=0): return pad_sequence([torch.tensor(t) for t in x], True, padding_value=p)\n",
    "def stack_wo_pad(x): return torch.tensor(x)\n",
    "\n",
    "def load_data(path):\n",
    "    logger.info(f'Loading dataset from {path}')\n",
    "\n",
    "    data = load_from_disk(path)\n",
    "    train, val = data['train'], data['val']\n",
    "\n",
    "    logger.info(f'Rows in train dataset: {len(train)}, rows in val dataset: {len(val)}')\n",
    "\n",
    "    return train, val\n",
    "\n",
    "def update_model(model, unfreeze_layers=0):\n",
    "    logger.info('Updating the model')\n",
    "\n",
    "    model.config.num_labels = num_classes\n",
    "    model.config.id2label = id2label\n",
    "    model.config.label2id = label2id\n",
    "\n",
    "    classifier_layer = torch.nn.Linear(\n",
    "        model.classifier.in_features,\n",
    "        num_classes\n",
    "    ).to('cuda')\n",
    "\n",
    "    model.classifier = classifier_layer\n",
    "    model.num_labels = num_classes\n",
    "\n",
    "    for name, layer in model.named_parameters():\n",
    "        layer.requires_grad = False\n",
    "\n",
    "    if unfreeze_layers > 0:\n",
    "        for layer in model.deberta.encoder.layer[-unfreeze_layers:].parameters():\n",
    "            layer.requires_grad = True\n",
    "\n",
    "    for layer in model.classifier.parameters():\n",
    "        layer.requires_grad = True\n",
    "\n",
    "    for name, layer in model.named_parameters():\n",
    "        if layer.requires_grad == True:\n",
    "            logger.info(f'Layer: {name} will be trained with dtype {layer.dtype}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def eval_model(trained_model, eval_dataset, bs):\n",
    "    label_metrics = dict.fromkeys(label2id.values())\n",
    "    for k, v in label_metrics.items():\n",
    "        label_metrics[k] = {'total_samples': 0,\n",
    "                            'total_predicted': 0, 'correct_predictions': 0}\n",
    "\n",
    "    trained_model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for s in tqdm(range(0, len(eval_dataset), bs)):\n",
    "            batch = eval_dataset[s:s+bs]\n",
    "\n",
    "            input_ids = stack_wo_pad(batch['input_ids']).to(device)\n",
    "            attention_mask = stack_wo_pad(batch['attention_mask']).to(device)\n",
    "            labels = stack_wo_pad(batch['labels']).to(device)\n",
    "\n",
    "            outputs = trained_model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs.logits, -1)\n",
    "\n",
    "            for p, l in zip(predicted_labels.flatten(), labels.flatten()):\n",
    "\n",
    "                if l == -100:\n",
    "                    continue\n",
    "\n",
    "                if p == l:\n",
    "                    correct_predictions += 1\n",
    "                    label_metrics[l.item()]['correct_predictions'] += 1\n",
    "\n",
    "                label_metrics[l.item()]['total_samples'] += 1\n",
    "                label_metrics[p.item()]['total_predicted'] += 1\n",
    "\n",
    "                total_samples += 1\n",
    "\n",
    "    label_metrics = pd.DataFrame.from_records(label_metrics).T\n",
    "    logger.info(f'Eval metrics: {label_metrics}')\n",
    "\n",
    "    return label_metrics\n",
    "\n",
    "def get_score(df, beta=5):\n",
    "    tp = df['correct_predictions'].sum()\n",
    "    fp = df['total_predicted'].sum() - tp\n",
    "    fn = df['total_samples'].sum() - tp\n",
    "    \n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    micro_fbeta_score = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "\n",
    "    return micro_fbeta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd590b8-3f6a-4be2-aac5-dcaeda9826fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0831d0fd-98d0-49f9-8a7a-f84a96ea4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "learning_rate = 3e-5\n",
    "batch_size = 4\n",
    "model_id = 'sileod/deberta-v3-large-tasksource-nli'\n",
    "# model_id = 'microsoft/deberta-v3-base'\n",
    "dataset = './data/processed/dataset_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b1709-f3a3-4524-88ce-2109d1250f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, val = load_data(path=dataset)\n",
    "model, tokenizer = get_model(model_id=model_id)\n",
    "\n",
    "model = update_model(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1, verbose=True)\n",
    "total_steps = len(train) * num_epochs\n",
    "\n",
    "device = 'cuda'\n",
    "loss_fn = CrossEntropyLoss(\n",
    "    # weight=torch.tensor([1, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]).to('cuda', dtype=torch.bfloat16),\n",
    "    # label_smoothing=0.05,\n",
    "    ignore_index=-100\n",
    ")\n",
    "\n",
    "all_losses = []\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5b7de-5822-4d2c-a612-49e45c0d85bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    with tqdm(total=len(train)//batch_size, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
    "        for s in range(0, len(train), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            batch = train[s:s+batch_size]\n",
    "\n",
    "            input_ids = stack_wo_pad(batch['input_ids']).to(device)\n",
    "            attention_mask = stack_wo_pad(batch['attention_mask']).to(device)\n",
    "            labels = stack_wo_pad(batch['labels']).to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            targets_flat = labels.view(-1)\n",
    "\n",
    "            loss = loss_fn(logits_flat, targets_flat)\n",
    "\n",
    "            loss.backward()\n",
    "            all_losses.append(loss.detach())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "            pbar.update(1)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a8a04-81af-4c79-acf7-c7ebec0c9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics = eval_model(model, train, batch_size)\n",
    "val_metrics = eval_model(model, val, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c72b0-9d0e-48f7-ab18-9cc632538883",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"./model/{datetime.strftime(datetime.now(), '%Y%m%d_%H%M')}\"\n",
    "model.save_pretrained(os.path.join(save_path, 'model'))\n",
    "tokenizer.save_pretrained(os.path.join(save_path, 'tokenizer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf9456-389a-4e55-8377-df4c2b194b9b",
   "metadata": {},
   "source": [
    "### Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e0b5f-44be-4a06-b638-0176a9fbb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from torch.cuda import empty_cache\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from data_utils import label2id, id2label\n",
    "from datasets import load_from_disk\n",
    "from model import get_model\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b55bf1-3837-408e-aa6b-e66b58d579a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1785152-8feb-45f2-bc96-971905830fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(model_path, tokenizer_path):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    model.to(device)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bc178-dfbe-4126-ae5a-c5a8890ab73a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m, t = get_trained_model('./model/20240407_1032/model/', './model/20240407_1032/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e909ed-99c4-40ae-a3ad-7d1e6b4ea5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk('./data/processed/dataset_4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519effba-77a7-4a49-a0f9-239d884e4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_single_ex(ex):\n",
    "    with torch.no_grad():\n",
    "        ii = torch.tensor(ex['input_ids']).reshape(1, -1).to(device)\n",
    "        am = torch.tensor(ex['attention_mask']).reshape(1, -1).to(device)\n",
    "\n",
    "        o = m(\n",
    "            input_ids=ii,\n",
    "            attention_mask=am\n",
    "        )\n",
    "\n",
    "        i = 0\n",
    "        # Predicted tokens\n",
    "        print('Predicted tokens')\n",
    "        for pred, token, label in zip(o.logits.argmax(-1)[0], ex['input_ids'], ex['labels']):\n",
    "            if t.decode(token) == '[PAD]':\n",
    "                i += 1\n",
    "                continue\n",
    "            if pred.item() == 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            print(f'{i} {t.decode(token)} ==> Predicted: {id2label[pred.item()]}, True: {id2label[label]}')\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        print('Expected tokens')\n",
    "        for pred, token, label in zip(o.logits.argmax(-1)[0], ex['input_ids'], ex['labels']):\n",
    "            if t.decode(token) == '[PAD]':\n",
    "                i += 1\n",
    "                continue\n",
    "            if label == 0 or label == -100:\n",
    "                i += 1\n",
    "                continue\n",
    "            print(f'{i} {t.decode(token)} ==> Predicted: {id2label[pred.item()]}, True: {id2label[label]}')\n",
    "            i += 1\n",
    "\n",
    "    empty_cache()\n",
    "\n",
    "    return o.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ba44e-6cb4-4bcd-b066-ed7d304dca48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = eval_single_ex(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408e2bba-9f22-4d77-9ebb-137f8f578a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = torch.nn.functional.softmax(result[0], dim=-1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0e5b9-3d5d-460b-9071-5726459e9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sm[:25].cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f7fdc-e00a-4210-9e08-68f84aad6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metrics = eval_model(m, ds['val'], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0c1ef7-89fd-49e0-835d-b0c4c501ccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_score(val_metrics[1:], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d8939-5dfe-4aa7-88e3-cc4c45775633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac24793c-c8c8-44cd-95c5-4f08036da941",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
